{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasilandrian/vasilandrian-/blob/main/IPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfqV2jxUgNWn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import requests\n",
        "import os\n",
        "from io import BytesIO\n",
        "import gzip\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from models_ddos import DDoSModel\n",
        "\n",
        "epochs = 100\n",
        "nclass = 12\n",
        "\n",
        "def loadDataset():\n",
        "    # Τοποθετήστε τον διαδρομή του dataset εδώ!\n",
        "    filename = 'https://raw.githubusercontent.com/kdemertzis/EKPA/main/Data/pcap_data.csv'\n",
        "\n",
        "    trainfile = pd.read_csv(filename)\n",
        "    data = pd.DataFrame(trainfile).to_numpy()\n",
        "    data = data[data[:, 67] != 'DrDoS_LDAP']\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    label = data[:, 67].astype('str')\n",
        "\n",
        "    label[label == 'WebDDoS'] = 0\n",
        "    label[label == 'BENIGN'] = 1\n",
        "    label[label == 'UDP-lag'] = 2\n",
        "    label[label == 'DrDoS_NTP'] = 3\n",
        "    label[label == 'Syn'] = 4\n",
        "    label[label == 'DrDoS_SSDP'] = 5\n",
        "    label[label == 'DrDoS_UDP'] = 6\n",
        "    label[label == 'DrDoS_NetBIOS'] = 7\n",
        "    label[label == 'DrDoS_MSSQL'] = 8\n",
        "    label[label == 'DrDoS_SNMP'] = 9\n",
        "    label[label == 'TFTP'] = 10\n",
        "    label[label == 'DrDoS_DNS'] = 11\n",
        "\n",
        "    inx_sel = -1 + np.array([38, 47, 37, 48, 11, 9, 7, 52, 10, 36, 1, 34, 4, 17, 19, 57, 21,\n",
        "                             18, 22, 24, 32, 50, 23, 55, 51, 5, 3, 39, 40, 43, 58, 12, 25,\n",
        "                             20, 2, 35, 67, 33, 6, 53])\n",
        "\n",
        "    data = data[:, inx_sel]\n",
        "    dmin = data.min(axis=0)\n",
        "    dmax = data.max(axis=0)\n",
        "    data = (data - dmin) / (dmax - dmin)\n",
        "\n",
        "    train_data, test_data, train_label, test_label = \\\n",
        "        train_test_split(data, label, test_size=0.20, stratify=label)\n",
        "\n",
        "    train_data, val_data, train_label, val_label = \\\n",
        "        train_test_split(train_data, train_label, test_size=0.125, stratify=train_label)\n",
        "\n",
        "    return train_data.astype('float32'), train_label.astype('int32'), \\\n",
        "           val_data.astype('float32'), val_label.astype('int32'), \\\n",
        "           test_data.astype('float32'), test_label.astype('int32')\n",
        "\n",
        "train_data, train_labelp, val_data, val_labelp, test_data, test_labelp = loadDataset()\n",
        "\n",
        "train_label = to_categorical(train_labelp, nclass)\n",
        "val_label = to_categorical(val_labelp, nclass)\n",
        "test_label = to_categorical(test_labelp, nclass)\n",
        "\n",
        "print('train_data.shape=', train_data.shape)\n",
        "print('test_data.shape=',  test_data.shape)\n",
        "print('val_data.shape=',  val_data.shape)\n",
        "\n",
        "inshape = train_data.shape[1]\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                  classes=np.unique(train_labelp),\n",
        "                                                  y=train_labelp)\n",
        "\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss',\n",
        "                              patience=30,\n",
        "                              verbose=0,\n",
        "                              mode='min')\n",
        "\n",
        "modelCheckPoint = ModelCheckpoint('./savemodels/model5class.weights.{epoch:03d}-{val_acc:.4f}.hdf5',\n",
        "                                  save_best_only=True,\n",
        "                                  monitor='val_acc',\n",
        "                                  mode='max')\n",
        "\n",
        "model = DDoSModel.model_conv1D(lr=1e-4, N=64, inshape=inshape)\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    train_label,\n",
        "                    shuffle=True,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=256,\n",
        "                    validation_data=(val_data, val_label),\n",
        "                    callbacks=[modelCheckPoint],\n",
        "                    class_weight=class_weights,\n",
        "                    workers=3)\n",
        "\n",
        "str_models = os.listdir('./savemodels')\n",
        "str_models = np.sort(str_models)\n",
        "best_model = str_models[str_models.size-1]\n",
        "print('best_model=', best_model)\n",
        "model.load_weights('./savemodels/'+best_model)\n",
        "\n",
        "pred = model.predict(test_data)\n",
        "pred_y = pred.argmax(axis=-1)\n",
        "\n",
        "cm = confusion_matrix(test_labelp.astype('int32'), pred_y)\n",
        "print(cm)\n",
        "\n",
        "label = np.array([\"WebDDoS\", \"BENIGN\", \"UDP-lag\", \"DrDoS_NTP\", \"Syn\",\n",
        "                  \"DrDoS_SSDP\", \"DrDoS_UDP\", \"DrDoS_NetBIOS\", \"DrDoS_MSSQL\",\n",
        "                  \"DrDoS_SNMP\", \"TFTP\", \"DrDoS_DNS\"])\n",
        "\n",
        "cmo = ConfusionMatrixDisplay(cm, display_labels=label)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "cmo.plot(ax=ax, xticks_rotation=45)\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "np.save('historydata.npy', [acc, val_acc, loss, val_loss])\n",
        "[acc, val_acc, loss, val_loss] = np.load('historydata.npy')\n",
        "\n",
        "plt.figure()\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}